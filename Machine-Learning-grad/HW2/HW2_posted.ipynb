{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b8b462",
   "metadata": {
    "id": "49b8b462"
   },
   "source": [
    "## MATH 4/5388: Machine Learning Methods\n",
    "## Homework 2\n",
    "### Due date: Tuesday, February 18\n",
    "\n",
    "#### Submission Instruction:\n",
    "* Submit both the Jupyter notebook file (.ipynb) and a PDF copy of the notebook.\n",
    "* Ensure that your notebook runs properly before submitting it:\n",
    "    * Kernel -> Restart & Run All to ensure that there are no errors.\n",
    "* To generate a PDF of your notebook:\n",
    "    * File -> Print Preview followed by printing to a PDF from your browser;\n",
    "    or:\n",
    "    * File -> Download as -> PDF via LaTeX.\n",
    "* If this doesn’t work, try first exporting as an HTML file and then converting that to PDF (load it in a web browser and print it to PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d4f89",
   "metadata": {
    "id": "c26d4f89"
   },
   "source": [
    "---\n",
    "### Problem 1 (20 points)\n",
    "\n",
    "The goal of this problem is to ensure that you understand the mathematical principles behind linear regression. Assume that we are given a training dataset consisting of inputs $\\mathbf{x}_n$ and targets $y_n$, $n=1,\\ldots,N$. Each $\\mathbf{x}_n$ is a D-dimensional vector consisting of elements $(x_{n,1}, x_{n,2}, \\ldots, x_{n,D})$ and $y_n\\in\\mathbb{R}$.\n",
    "\n",
    "\n",
    "**1a. Linear Regression Prediction Function**\n",
    "- Explain the form of the prediction function for a linear regression model $f(\\mathbf{x}_n; \\boldsymbol{\\theta})$. Assume we have a set of coefficients represented by a D-dimensional vector $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\ldots, \\theta_D)$. For simplicity, we ignore the bias or intercept term of the linear regression model in this analysis.\n",
    "\n",
    "\n",
    "**1b. Residual Sum of Squares (RSS) in Compact Form**\n",
    "- Substitute the prediction function from **1a** into the Residual Sum of Squares (RSS) derived in Homework 1. Show that this expression can be rewritten in the form of matrices and vectors, eliminating the explicit summation.\n",
    "  \n",
    "\\begin{equation*}\n",
    " \\text{RSS}(\\boldsymbol{\\theta}) = \\sum_{n=1}^N \\big(y_n - f(\\mathbf{x}_n; \\boldsymbol{\\theta)}\\big)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9288474-a4fa-4e65-9ae4-9b5a0d0ca15e",
   "metadata": {},
   "source": [
    "![Solution1](Question1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744964d6",
   "metadata": {
    "id": "744964d6"
   },
   "source": [
    "---\n",
    "### Problem 2 (20 points)\n",
    "\n",
    "In this problem, you will work with the **mtcars** dataset, which is loaded for you in the provided cell.\n",
    "\n",
    "**2a. Data Splitting**\n",
    "- Split the dataset into training and testing sets.  \n",
    "- Use 'hp' (horsepower) and 'wt' (weight) as input features, and 'mpg' (miles per gallon) as the target variable.  \n",
    "- After splitting, print the size of the training and testing sets.\n",
    "\n",
    "**2b. Model Fitting**  \n",
    "Using scikit-learn, fit a linear regression model to the training data for the following four cases:  \n",
    "- Case 1: Use 'hp' (horsepower) as the only input feature.  \n",
    "- Case 2: Use 'wt' (weight) as the only input feature.  \n",
    "- Case 3: Use both 'hp' and 'wt' as input features without any preprocessing.\n",
    "- Case 4: Use both 'hp' and 'wt' as input features with preprocessing.  Choose one preprocessing technique from `sklearn.preprocessing` (e.g., `StandardScaler`, `MinMaxScaler`, `PolynomialFeatures`, etc.) and apply it before fitting the model.\n",
    "\n",
    "**2c. Model Evaluation**  \n",
    "- Evaluate the performance of each model on the test set using at least two different metrics from `sklearn.metrics`.  \n",
    "- Present the evaluation results clearly for each case.\n",
    "\n",
    "**2d. Analysis**  \n",
    "- Compare the performance of Case 3 (without preprocessing) and Case 4 (with preprocessing).  \n",
    "- Discuss whether preprocessing improved the model’s performance when using both features. Support your answer using the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892c1853",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 25119,
     "status": "ok",
     "timestamp": 1739562403713,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "892c1853",
    "outputId": "1aa2a6c2-746e-405d-cae0-4ee05f5050e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load mtcars\n",
    "df = pd.read_csv('mtcars.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hrG7QZmTodIF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1369,
     "status": "ok",
     "timestamp": 1739487390281,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "hrG7QZmTodIF",
    "outputId": "308d2a0d-65c4-4ef5-db50-c2af16fe1272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: 24\n",
      "y_train size: 24\n",
      "X_test size: 8\n",
      "y_test size: 8\n"
     ]
    }
   ],
   "source": [
    "# 2a\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_features = ['hp', 'wt']\n",
    "target = 'mpg'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[input_features], df[target],\n",
    "test_size=0.25,\n",
    "random_state=5)\n",
    "\n",
    "print(f'X_train size: {X_train.shape[0]}')\n",
    "print(f'y_train size: {y_train.shape[0]}')\n",
    "print(f'X_test size: {X_test.shape[0]}')\n",
    "print(f'y_test size: {y_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "I3G7n03OogWV",
   "metadata": {
    "id": "I3G7n03OogWV"
   },
   "outputs": [],
   "source": [
    "# 2b\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Case 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[input_features[0]]], df[target],\n",
    "test_size=0.25,\n",
    "random_state=5)\n",
    "\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_test)\n",
    "\n",
    "# Case 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[input_features[1]]], df[target],\n",
    "test_size=0.25,\n",
    "random_state=5)\n",
    "\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "\n",
    "# Case 3\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[input_features], df[target],\n",
    "test_size=0.25,\n",
    "random_state=5)\n",
    "\n",
    "model3 = LinearRegression()\n",
    "model3.fit(X_train, y_train)\n",
    "y_pred3 = model3.predict(X_test)\n",
    "\n",
    "# Case 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[input_features], df[target],\n",
    "test_size=0.25,\n",
    "random_state=5)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized = scaler.fit_transform(X_train)\n",
    "X_train_scale = scaler.transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model4 = LinearRegression()\n",
    "model4.fit(X_train_scale, y_train)\n",
    "y_pred4 = model4.predict(X_test_scale)\n",
    "\n",
    "# Case 4 2.0\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[input_features], df[target],\n",
    "test_size=0.25,\n",
    "random_state=5)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "model5 = LinearRegression()\n",
    "model5.fit(X_train_scale, y_train)\n",
    "y_pred5 = model5.predict(X_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "x8KTSRndoh2u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739487390641,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "x8KTSRndoh2u",
    "outputId": "f2c66530-588e-44e0-f758-623c39728b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for case 1\n",
      "R2 score: 0.5299294117029201\n",
      "Mean squared error: 13.079053082601451\n",
      "Metrics for case 2\n",
      "R2 score: 0.687880928508651\n",
      "Mean squared error: 8.684274246802499\n",
      "Metrics for case 3\n",
      "R2 score: 0.701160120459662\n",
      "Mean squared error: 8.314799404629298\n",
      "Metrics for case 4\n",
      "R2 score: 0.701160120459662\n",
      "Mean squared error: 8.3147994046293\n",
      "Metrics for case 5\n",
      "R2 score: 0.7011601204596621\n",
      "Mean squared error: 8.314799404629296\n"
     ]
    }
   ],
   "source": [
    "# 2c\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "y_pred = [y_pred1, y_pred2, y_pred3, y_pred4, y_pred5]\n",
    "\n",
    "for i in range(5):\n",
    "  print(f'Metrics for case {i + 1}')\n",
    "  print(f'R2 score: {r2_score(y_test, y_pred[i])}')\n",
    "  print(f'Mean squared error: {mean_squared_error(y_test, y_pred[i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2zLL9mi9OWgj",
   "metadata": {
    "id": "2zLL9mi9OWgj"
   },
   "source": [
    "# 2d.\n",
    "Preprocessing didn't improve the performance of the model as seen from the metrics for cases 3-5 which are all identical. Using both features had an impact, as the R2 score for individual features was 0.53 and 0.69, while using both features was 0.70."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee2465",
   "metadata": {
    "id": "b7ee2465"
   },
   "source": [
    "---\n",
    "### Problem 3 (30 points)\n",
    "\n",
    "In this problem, you will use the California Housing dataset from `sklearn.datasets` to practice model selection and cross-validation techniques. Your goal is to predict housing prices based on various features in the dataset. The following figure illustrates the general approach that we are taking in this problem.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" width=350>\n",
    "\n",
    "**3a. Load and Explore the Data**\n",
    "- Explore the dataset: display the first few rows, summarize the features and target variable (e.g., number of samples $N$, number of features $D$, etc.)\n",
    "- Split the data into training and testing sets.\n",
    "\n",
    "**3b. Define a Pipeline**\n",
    "\n",
    "- Create a pipeline that includes preprocessing step (standardize the input features using StandardScaler), use PolynomialFeatures to transform the features, and followed by `SGDRegressor` (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html).\n",
    "\n",
    "**3c. Cross-Validation for Model Selection**\n",
    "\n",
    "- Use cross-validation on the training set to determine the best degree for the polynomial features.\n",
    "- Test at least degrees 1 through 4.\n",
    "- Evaluate model performance using Mean Squared Error (MSE) as the scoring metric.\n",
    "- Report the average cross-validation score for each polynomial degree.\n",
    "\n",
    "**3d. Final Model Evaluation**\n",
    "- Based on your cross-validation results, select the polynomial degree with the best performance.\n",
    "- Retrain the pipeline using the entire training set with the selected degree.\n",
    "- Evaluate the final model on the test set using the following metrics from `sklearn.metrics`: Mean Squared Error (MSE) and R-squared.  \n",
    "- How well does the model generalize to unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf219b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1739487392292,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "bbf219b1",
    "outputId": "30c261b3-8e41-47f7-fe11-cda746e26ffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9rAxeu9lOVOJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739487392292,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "9rAxeu9lOVOJ",
    "outputId": "c701bef9-afa6-4396-fda2-678264825db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "<bound method NDFrame.describe of        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  \n",
      "0        -122.23  \n",
      "1        -122.22  \n",
      "2        -122.24  \n",
      "3        -122.25  \n",
      "4        -122.25  \n",
      "...          ...  \n",
      "20635    -121.09  \n",
      "20636    -121.21  \n",
      "20637    -121.22  \n",
      "20638    -121.32  \n",
      "20639    -121.24  \n",
      "\n",
      "[20640 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "# 3a\n",
    "features = housing.data\n",
    "target = housing.target\n",
    "print(housing.DESCR)\n",
    "print(pd.DataFrame(housing.data, columns=housing.feature_names).describe)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "SXSSiAjsTlfq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9784,
     "status": "ok",
     "timestamp": 1739487402071,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "SXSSiAjsTlfq",
    "outputId": "9d9e77e7-2cb4-4502-ef8e-6cadeca645b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for degree 1: -0.5320564118463933\n",
      "Average score for degree 2: -7.497742208565386e+26\n",
      "Average score for degree 3: -5.046291521646477e+35\n",
      "Average score for degree 4: -8.808015554904268e+43\n",
      "Average score for degree 5: -6.854005066072789e+51\n",
      "Best degree: 1\n"
     ]
    }
   ],
   "source": [
    "# 3b/c\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "best_degree = 1\n",
    "best_score = -np.inf\n",
    "\n",
    "for degrees in range(1, 6):\n",
    "  pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree = degrees)),\n",
    "    ('sgd_reg', SGDRegressor(alpha=0.01, tol=0.0001, eta0=0.001, max_iter=10000))\n",
    "])\n",
    "\n",
    "  pipeline.fit(X_train, y_train)\n",
    "\n",
    "  cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "  avg_cv = np.mean(cv_scores)\n",
    "\n",
    "\n",
    "  if avg_cv > best_score:\n",
    "    best_score = avg_cv\n",
    "    best_degree = degrees\n",
    "\n",
    "  print(f'Average score for degree {degrees}: {avg_cv}')\n",
    "\n",
    "print(f'Best degree: {best_degree}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "E8WDcwBg3Qr8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1739487402072,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "E8WDcwBg3Qr8",
    "outputId": "180a001c-9342-4275-cf12-a62265108ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.5077250925680745\n",
      "Mean squared error: 0.6444655553035359\n"
     ]
    }
   ],
   "source": [
    "# 3d\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree = 1)),\n",
    "    ('sgd_reg', SGDRegressor())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'R2 score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtBJjTfR6X02",
   "metadata": {
    "id": "RtBJjTfR6X02"
   },
   "source": [
    "The scores reflect the model did moderately well generalizing unseen data, especially considering the size of the data/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec370de4",
   "metadata": {
    "id": "ec370de4"
   },
   "source": [
    "---\n",
    "### Problem 4 (30 points)\n",
    "\n",
    "In this problem, you will explore the relationship between explained variance and the $R^2$ score. You will start with empirical experiments [4388 and 5388] to observe different scenarios and then proceed to a theoretical proof [5388 only]. Recall the definitions for explained variance and the $R^2$ score given a set of ground-truth outputs $y_1,\\ldots,y_N$  and predicted values $\\hat{y}_1,\\ldots, \\hat{y}_N$:\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\text{EV} = 1 - \\frac{\\text{Var}(y - \\hat{y})}{\\text{Var}(y)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    " R^2 = 1 - \\frac{\\sum_{n=1}^N (y_n - \\hat{y}_n)^2}{\\sum_{i=n}^N (y_n - \\bar{y})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "**4a. Empirical analysis [4388 and 5388]**\n",
    "- Write a Python script to explore the relationship between explained variance and the $R^2$ score.\n",
    "- Provide different scenarios in your experiment:\n",
    "    - Scenario 1: Perfect predictions.\n",
    "    - Scenario 2: Predictions with zero-mean residuals.\n",
    "    - Scenario 3: Predictions with biased residuals.\n",
    "- For each scenario, use `sklearn.metrics.explained_variance_score` and `sklearn.metrics.r2_score`.\n",
    "\n",
    "**4b. Theoretical analysis [5388]**\n",
    "- Prove under what conditions the explained variance reduces to the $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037bb61f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1739490579272,
     "user": {
      "displayName": "Andrew",
      "userId": "03132461877583117226"
     },
     "user_tz": 420
    },
    "id": "037bb61f",
    "outputId": "6958fe7d-f47f-4f49-fca2-d21b050e4d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance for case 1: 1.0\n",
      "R2 score for case 1: 1.0\n",
      "Explained variance for case 2: 0.0\n",
      "R2 score for case 2: 0.0\n",
      "Explained variance for case 3(alternating): -102.72727272727272\n",
      "R2 score for case 3: -105.22727272727273\n",
      "Explained variance for case 3(-10s): 0.0\n",
      "R2 score for case 3: -22.5\n",
      "Explained variance for case 3(200s): 0.0\n",
      "R2 score for case 3: -3802.5\n"
     ]
    }
   ],
   "source": [
    "# 4a\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "# Scenario 1:\n",
    "y_true = y_pred = np.arange(0, 11)\n",
    "print(f'Explained variance for case 1: {explained_variance_score(y_true, y_pred)}')\n",
    "print(f'R2 score for case 1: {r2_score(y_true, y_pred)}')\n",
    "\n",
    "# Scenario 2:\n",
    "y_true = np.arange(0, 11)\n",
    "y_pred = [5] * 11\n",
    "print(f'Explained variance for case 2: {explained_variance_score(y_true, y_pred)}')\n",
    "print(f'R2 score for case 2: {r2_score(y_true, y_pred)}')\n",
    "\n",
    "# Scenario 3:\n",
    "y_true = np.arange(0, 11)\n",
    "y_pred = [50, -50, 40, -40, 30, -30, 20, -20, 10, -10, 0]\n",
    "print(f'Explained variance for case 3(alternating): {explained_variance_score(y_true, y_pred)}')\n",
    "print(f'R2 score for case 3: {r2_score(y_true, y_pred)}')\n",
    "\n",
    "y_true = np.arange(0, 11)\n",
    "y_pred = [-10] * 11\n",
    "print(f'Explained variance for case 3(-10s): {explained_variance_score(y_true, y_pred)}')\n",
    "print(f'R2 score for case 3: {r2_score(y_true, y_pred)}')\n",
    "\n",
    "y_true = np.arange(0, 11)\n",
    "y_pred = [200] * 11\n",
    "print(f'Explained variance for case 3(200s): {explained_variance_score(y_true, y_pred)}')\n",
    "print(f'R2 score for case 3: {r2_score(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4e261-f5e5-42d0-8add-c06fbd89aa37",
   "metadata": {},
   "source": [
    "![Solution4](Question4.jpg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
